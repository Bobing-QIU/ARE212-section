{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87345c22",
   "metadata": {},
   "source": [
    "# Kernel regression\n",
    "\n",
    "Last section we looked at kernel density estimators, or ways of approximating the probability density function of a single random variable: $f(x)$. Now we want to do something a bit more ambitious: estimate the expectation of a random variable $Y$ conditional on another random variable $X$: $E(Y|X)$. We know from Sofia's class that the best _linear approximation_ of this conditional expectation is equal to $(X'X)^{-1}X'Y$. But what if we think the relationship is non-linear, and want to get this right? Then we can use kernel regression.\n",
    "\n",
    "Formally, suppose we have a true model of the shape:\n",
    "\n",
    "$$y = m(X) + e$$\n",
    "\n",
    "but we don't know the shape of the function $m()$. If we assume that $E(e|X) = 0$, then we can derive the kernel regression estimator (see Ethan's notes for math):\n",
    "\n",
    "$$\\widehat{m}(x) = \\frac{\\sum_i k(\\frac{X_i-x}{h})y_i}{\\sum_i k(\\frac{X_i-x}{h})}$$\n",
    "\n",
    "You can program this similarly as we did before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import distributions as iid\n",
    "from numpy import random\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b44865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define true conditional expectation\n",
    "def m(x): \n",
    "    return np.sin(2*x)/(1+x)\n",
    "\n",
    "# create DGP\n",
    "def dgp(N, m):\n",
    "    '''\n",
    "    Generate X, Y data.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    N (int): Number of observations\n",
    "    m (function): true conditional expectation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X (pd.Series): draws of independent variable\n",
    "    y (pd.Series): draws of dependent variable\n",
    "    '''\n",
    "    # X data\n",
    "    rvX = iid.uniform(0, 2*np.pi)\n",
    "    X = pd.Series(rvX.rvs(size=N))\n",
    "    # sort\n",
    "    X = X.sort_values()\n",
    "    \n",
    "    # noise\n",
    "    E = iid.norm(scale=0.1)\n",
    "    e = E.rvs(size=N)\n",
    "    \n",
    "    # create Y\n",
    "    y = m(X) + e\n",
    "    \n",
    "    return y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data and plot\n",
    "y, X = dgp(100, m)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(X, y, label=\"Data observations of x and y\")\n",
    "plt.plot(X, m(X), label='True E(y|X)', color='orange')\n",
    "ax.set(xlabel='X Values', ylabel='y values')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcebfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(u):\n",
    "    '''\n",
    "    Gaussian kernel density at point x given data X.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    u (float): point at which to evaluate the density: x-X/h\n",
    "    X (pd.Series): data to use\n",
    "    h (float): bandwidth\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    k(u) (pd.Series): kernel \n",
    "    '''\n",
    "    return (1/(2*np.pi))*np.exp(-u**2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mhat(x, k, X, h, y):\n",
    "    '''\n",
    "    Kernel regression\n",
    "    \n",
    "    Inputs\n",
    "    -----\n",
    "    x (float): point at which to estimate the kernel regression\n",
    "    k (function): kernel function to use; function of u only.\n",
    "    X (pd.Series): data\n",
    "    y (pd.Series): data outcome variable\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mhat(x) (float): kernel regression estimate\n",
    "    \n",
    "    '''\n",
    "    u = (x-X)/h\n",
    "    return (k(u)*y).sum()/k(u).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ebd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate kernel regression for 2 different bandwidths\n",
    "hvals = [0.05, 0.5, 2]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,7))\n",
    "counter = 0\n",
    "\n",
    "for bandwidth in hvals:\n",
    "    # calculate m hat\n",
    "    m = X.apply(lambda x: mhat(x, K, X, bandwidth, y))\n",
    "    \n",
    "    # plot mhat \n",
    "    ax[counter].plot(X, m, label = 'mhat', color='orange')\n",
    "    \n",
    "    # plot points for reference\n",
    "    ax[counter].scatter(X, y, label = 'data')\n",
    "    \n",
    "    # title and legend\n",
    "    ax[counter].set_title(f'h = {bandwidth}')\n",
    "    ax[counter].legend()\n",
    "    \n",
    "    # advance index for next plot\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203a7d7",
   "metadata": {},
   "source": [
    "# Overfitting and how to judge how good we're doing\n",
    "\n",
    "How can we evaluate how well this estimator is doing? By construction, it is doing the best estimation it can at each of the $x$ in our data, given our bandwidth. We can see that the small bandwidth is doing a little too well- it is hitting a lot of our points, but it is achieving this by overfitting the noise in the data. This means that given a new $x$ that wasn't in our data, this estimator wouldn't give an accurate idea of what the conditional expectation of $y$ is given a new data point. \n",
    "\n",
    "This intuition shows us why calulating the estimated mean square error isn't a great way to evaluate our estimator- mechanically, if we make the bandwidth smaller and smaller, we will get smaller errors but we won't necessarily do a good job of getting the overall shape right. In order to evaluate how well we would do given a new point, we need to so how close $\\widehat{m}(x)$ gets given a new $x$ that wasn't used in the initial calculation. \n",
    "\n",
    "We can do this by doing something clever with our estimation: we can leave one of our data points $x_{i}$ out of the estimation of $m()$, call this $\\widehat{m}_{-i}$, then evaluate how well we do by calculting $\\widehat{m}(x_i)$. Then we can see how large the \"out of sample\" error $e_{-i} = y_i-\\widehat{m}_{-i}(x_i)$. We can do this one-by-one for each $x_i$ in the data, then define the \"leave-one-out\" estimated mean squared error as:\n",
    "\n",
    "$$EMSE = \\frac{1}{n}\\sum_i e_{-i}^2$$\n",
    "\n",
    "## What the... what's a Gram matrix?\n",
    "A gram matrix is just a convenient way to calculate all of these $\\widehat{m}_{-i}$! Define the gram matrix $G$ as:\n",
    "\n",
    "$$G = \\begin{bmatrix}\n",
    "k(\\frac{x_1-x_1}{h}) & k(\\frac{x_1-x_2}{h}) & \\ldots  & k(\\frac{x_1-x_N}{h})\\\\\n",
    "\\ldots & \\ldots  & \\ldots  & \\ldots \\\\\n",
    "k(\\frac{x_N-x_1}{h}) & k(\\frac{x_N-x_2}{h}) & \\ldots & k(\\frac{x_N-x_N}{h})\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and define:\n",
    "\n",
    "$$G_{-} = \\begin{bmatrix}\n",
    "0 & k(\\frac{x_1-x_2}{h}) & \\ldots  & k(\\frac{x_1-x_N}{h})\\\\\n",
    "\\ldots & 0  & \\ldots  & \\ldots \\\\\n",
    "k(\\frac{x_N-x_1}{h}) & k(\\frac{x_N-x_2}{h}) & \\ldots & 0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Throwing back to when we remembered what matrix algebra actually is, notice that:\n",
    "\n",
    "$$Gy = \\begin{bmatrix}\n",
    "\\sum_i k(\\frac{x_1-x_i}{h})y_i\\\\\n",
    "\\ldots \\\\\n",
    "\\sum_i k(\\frac{x_N-x_i}{h})y_i\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$G_{-}y = \\begin{bmatrix}\n",
    "\\sum_{i\\neq 1} k(\\frac{x_1-x_i}{h})y_i\\\\\n",
    "\\ldots \\\\\n",
    "\\sum_{i\\neq N} k(\\frac{x_N-x_i}{h})y_i\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "So the Gram matrix gives us an easy way of calculating all our $\\widehat{m}$ and $\\widehat{m}_{-i}$ terms!\n",
    "\n",
    "$$EMSE = \\frac{1}{N} \\sum y- \\widehat{m}_{-} = \\frac{1}{N} \\sum y- \\frac{G_{-}y}{G_{-}1_N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gminus(k, X, h):\n",
    "    '''\n",
    "    Gram matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    --------\n",
    "    k (function): kernel function, function of 1 input (u)\n",
    "    X (np.Series): data\n",
    "    h (float): bandwidth\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Gminus (np.array): NxN array of kernels\n",
    "    '''\n",
    "    X = np.array(X).reshape(-1,1)\n",
    "    u = (X.T - X)/h # NxN matrix of x_i - x_j\n",
    "    \n",
    "    # applied to each u, get G\n",
    "    G = k(u)\n",
    "    \n",
    "    # take out diagonal\n",
    "    Gminus = G - np.eye(u.shape[0])*np.diagonal(G)\n",
    "    return Gminus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b47f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mhat_minus(k, X, h, y):\n",
    "    denom = Gminus(k, X, h).sum(axis=0).reshape(-1,1)\n",
    "    numer = Gminus(k, X, h)@(np.array(y).reshape(-1,1))\n",
    "    return numer/denom\n",
    "\n",
    "def EMSE(k, X, h, y):\n",
    "    e = np.array(y).reshape(-1,1) - mhat_minus(k, X, h, y)\n",
    "    return (e**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86655e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMSE for different bandwidths\n",
    "\n",
    "h = np.linspace(0.01, 1, 100)\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,7))\n",
    "ax.plot(h, [EMSE(K, X, bandwidth, y) for bandwidth in h])\n",
    "\n",
    "plt.title(\"EMSE as a function of the bandwidth\", size=18)\n",
    "plt.xlabel('Bandwidth h', size=16)\n",
    "plt.ylabel('EMSE', size=16)\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
